#!/usr/bin/env python3
"""
ğŸ”´ VELES CRASH ANALYZER â€” CLI

ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€Ğ°ÑˆĞ° BTC Ñ 200+ Ñ„Ğ¸Ñ‡Ğ°Ğ¼Ğ¸,
Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (XGBoost/LightGBM/CatBoost) Ğ¸ SHAP.

ĞĞ´Ğ½Ğ° ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° â€” Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½:
  python scripts/crash_analysis/run.py

ĞĞ¿Ñ†Ğ¸Ğ¸:
  --symbol BTCUSDT     Ğ¡Ğ¸Ğ¼Ğ²Ğ¾Ğ» (default: BTCUSDT)
  --years 5            Ğ›ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (default: 5)
  --crash-pct 5.0      ĞŸĞ¾Ñ€Ğ¾Ğ³ ĞºÑ€Ğ°ÑˆĞ° Ğ² % (default: 5.0)
  --crash-window 48    ĞĞºĞ½Ğ¾ ĞºÑ€Ğ°ÑˆĞ° Ğ² Ğ±Ğ°Ñ€Ğ°Ñ… 15m (default: 48 = 12Ñ‡)
  --no-cache           ĞĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞµÑˆ
  --force-refresh      ĞŸĞµÑ€ĞµĞ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
  --fast               Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ (Ğ±ĞµĞ· complexity features, Ğ±ĞµĞ· Optuna)
  --optuna-trials 50   ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ trials Optuna (default: 50)
  --models xgboost,lightgbm,catboost
"""

import argparse
import sys
import time
from pathlib import Path

# Add parent dir to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))


def main() -> None:
    parser = argparse.ArgumentParser(
        description="ğŸ”´ Veles Crash Analyzer â€” Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ĞºÑ€Ğ°ÑˆĞµĞ¹ BTC",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument("--symbol", default="BTCUSDT", help="Ğ¢Ğ¾Ñ€Ğ³Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»")
    parser.add_argument("--years", type=int, default=5, help="Ğ›ĞµÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
    parser.add_argument("--crash-pct", type=float, default=5.0, help="ĞŸĞ¾Ñ€Ğ¾Ğ³ ĞºÑ€Ğ°ÑˆĞ° (%%)")
    parser.add_argument("--crash-window", type=int, default=48, help="ĞĞºĞ½Ğ¾ ĞºÑ€Ğ°ÑˆĞ° (Ğ±Ğ°Ñ€Ğ¾Ğ² 15m)")
    parser.add_argument("--no-cache", action="store_true", help="ĞĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞµÑˆ")
    parser.add_argument("--force-refresh", action="store_true", help="ĞŸĞµÑ€ĞµĞ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ")
    parser.add_argument("--fast", action="store_true", help="Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼")
    parser.add_argument("--optuna-trials", type=int, default=50, help="Optuna trials")
    parser.add_argument("--models", default="xgboost,lightgbm,catboost", help="ĞœĞ¾Ğ´ĞµĞ»Ğ¸ (Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¿ÑÑ‚ÑƒÑ)")
    parser.add_argument("--no-plots", action="store_true", help="ĞĞµ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸")

    args = parser.parse_args()

    start_time = time.time()

    print()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘              ğŸ”´ VELES CRASH ANALYZER                               â•‘")
    print("â•‘              ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€Ğ°ÑˆĞ°                    â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    print(f"   Symbol:       {args.symbol}")
    print(f"   Years:        {args.years}")
    print(f"   Crash:        â‰¥{args.crash_pct}% Ğ·Ğ° {args.crash_window} Ğ±Ğ°Ñ€Ğ¾Ğ² ({args.crash_window * 15 / 60:.0f}Ñ‡)")
    print(f"   Models:       {args.models}")
    print(f"   Mode:         {'FAST' if args.fast else 'FULL'}")
    print(f"   Optuna:       {'OFF' if args.fast else f'{args.optuna_trials} trials'}")
    print()

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 1: LOAD DATA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("=" * 70)
    print("ğŸ“¡ STEP 1/5: Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Binance")
    print("=" * 70)

    from crash_analysis.data_loader import load_all_data

    data = load_all_data(
        symbol=args.symbol,
        years=args.years,
        use_cache=not args.no_cache,
        force_refresh=args.force_refresh,
    )

    step1_time = time.time() - start_time
    print(f"\n   â±ï¸  Ğ¨Ğ°Ğ³ 1: {step1_time:.0f}Ñ")
    for key, df in data.items():
        print(f"   {key}: {len(df):,} ÑÑ‚Ñ€Ğ¾Ğº")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 2: FEATURE ENGINEERING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\n{'=' * 70}")
    print("ğŸ”§ STEP 2/5: Feature Engineering")
    print("=" * 70)

    from crash_analysis.features import build_features

    features_df = build_features(
        data=data,
        crash_threshold_pct=args.crash_pct,
        crash_window_bars=args.crash_window,
        include_complexity=not args.fast,
    )

    step2_time = time.time() - start_time - step1_time
    print(f"\n   â±ï¸  Ğ¨Ğ°Ğ³ 2: {step2_time:.0f}Ñ")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 3: ML PIPELINE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\n{'=' * 70}")
    print("ğŸ§  STEP 3/5: ML Training & Validation")
    print("=" * 70)

    from crash_analysis.models import ModelConfig, run_pipeline

    model_config = ModelConfig(
        models=args.models.split(","),
        run_optuna=not args.fast,
        optuna_n_trials=args.optuna_trials,
        optuna_timeout=600 if not args.fast else 60,
        use_ensemble=len(args.models.split(",")) >= 2,
    )

    pipeline_result = run_pipeline(features_df, model_config)

    step3_time = time.time() - start_time - step1_time - step2_time
    print(f"\n   â±ï¸  Ğ¨Ğ°Ğ³ 3: {step3_time:.0f}Ñ")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 4: ANALYSIS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\n{'=' * 70}")
    print("ğŸ” STEP 4/5: Post-Analysis")
    print("=" * 70)

    from crash_analysis.analysis import (
        analyze_by_regime,
        detect_anomalies,
        detect_regimes,
        extract_rules,
    )

    # Regime detection
    print("\n   ğŸ“Š Regime Detection...")
    df_15m = data.get("futures_15m")
    regimes = detect_regimes(df_15m) if df_15m is not None and len(df_15m) > 0 else None

    # Performance by regime
    regime_analysis = None
    if regimes is not None:
        print("\n   ğŸ“Š Performance by Regime...")
        regime_analysis = analyze_by_regime(pipeline_result.predictions, regimes)

    # Anomaly detection
    print("\n   ğŸ“Š Anomaly Detection...")
    anomalies = detect_anomalies(features_df)

    # Rule extraction
    print("\n   ğŸ“Š Rule Extraction...")
    rules = extract_rules(pipeline_result, top_n=20)
    for i, rule in enumerate(rules[:10], 1):
        print(f"   {i:2d}. {rule['feature']} ({rule['importance']:.4f})")
        print(f"       â†’ {rule['description']}")

    step4_time = time.time() - start_time - step1_time - step2_time - step3_time

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 5: REPORT & VISUALIZATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\n{'=' * 70}")
    print("ğŸ“Š STEP 5/5: Report & Visualization")
    print("=" * 70)

    from crash_analysis.analysis import generate_plots, generate_report

    # Report
    report_path = generate_report(pipeline_result, rules, regime_analysis)

    # Plots
    if not args.no_plots:
        print("\n   ğŸ“Š Generating plots...")
        try:
            plots = generate_plots(pipeline_result, data)
            print(f"   âœ… {len(plots)} Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ Ğ² output/")
        except Exception as e:
            print(f"   âš ï¸  Plot generation failed: {e}")

    # Save predictions
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)
    pred_path = output_dir / "predictions.csv"
    pipeline_result.predictions.to_csv(pred_path, index=False)
    print(f"   ğŸ’¾ Predictions: {pred_path}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SUMMARY
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    total_time = time.time() - start_time

    print(f"\n{'=' * 70}")
    print("âœ… DONE")
    print(f"{'=' * 70}")
    print(f"   ĞĞ±Ñ‰ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ:      {total_time:.0f}Ñ ({total_time / 60:.1f}Ğ¼Ğ¸Ğ½)")
    print(f"   Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ:           {step1_time:.0f}Ñ")
    print(f"   Features:         {step2_time:.0f}Ñ")
    print(f"   ML Training:      {step3_time:.0f}Ñ")
    print(f"   Analysis:         {step4_time:.0f}Ñ")
    print(f"   Best Model:       {pipeline_result.best_model_name}")
    best_auc = (
        pipeline_result.models.get(pipeline_result.best_model_name, next(iter(pipeline_result.models.values())))
        .metrics.get("roc_auc", 0)
    )
    print(f"   Best AUC:         {best_auc:.4f}")
    print(f"   Features:         {pipeline_result.n_features}")
    print(f"   Output:           output/")
    print()


if __name__ == "__main__":
    main()
