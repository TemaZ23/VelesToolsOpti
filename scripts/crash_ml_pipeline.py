#!/usr/bin/env python3
"""
BTC Crash Prediction ML Pipeline
=================================

Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ»Ğ¸ĞºĞ²Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ»Ğ¸Ğ²Ğ°Ğ¼ BTC.

Ğ¢Ğ¸Ğ¿ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Binary classification Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¾Ğ¼ ĞºĞ»Ğ°ÑÑĞ¾Ğ².
Ğ¢Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼: 15 Ğ¼Ğ¸Ğ½ÑƒÑ‚.

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
    1. Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ CSV Ğ¸Ğ· Veles Tools (Crash Analysis â†’ Export CSV)
    2. python crash_ml_pipeline.py --input crash_data.csv

Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸:
    pip install pandas numpy scikit-learn xgboost lightgbm shap matplotlib seaborn

ĞĞ²Ñ‚Ğ¾Ñ€: Veles Tools
"""

import argparse
import warnings
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    precision_recall_curve,
    roc_auc_score,
    roc_curve,
)
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings('ignore')

# ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ° Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº
try:
    import xgboost as xgb
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False
    print("âš ï¸  XGBoost Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½. Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ: pip install xgboost")

try:
    import lightgbm as lgb
    HAS_LIGHTGBM = True
except ImportError:
    HAS_LIGHTGBM = False
    print("âš ï¸  LightGBM Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½. Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ: pip install lightgbm")

try:
    import shap
    HAS_SHAP = True
except ImportError:
    HAS_SHAP = False
    print("âš ï¸  SHAP Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½. Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ: pip install shap")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class PipelineConfig:
    """ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ ML Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°"""
    # Target variable
    crash_threshold_pct: float = 5.0  # ĞŸĞ¾Ñ€Ğ¾Ğ³ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ½Ñ‹ Ğ´Ğ»Ñ crash
    crash_window_bars: int = 48       # ĞĞºĞ½Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° crash (48 Ğ±Ğ°Ñ€Ğ¾Ğ² = 12 Ñ‡Ğ°ÑĞ¾Ğ²)
    
    # Feature engineering windows
    zscore_24h_bars: int = 96         # 24h = 96 Ğ±Ğ°Ñ€Ğ¾Ğ² Ğ¿Ğ¾ 15m
    zscore_72h_bars: int = 288        # 72h = 288 Ğ±Ğ°Ñ€Ğ¾Ğ²
    zscore_7d_bars: int = 672         # 7d = 672 Ğ±Ğ°Ñ€Ğ°
    
    # Walk-forward validation
    train_years: int = 2              # Ğ›ĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
    test_months: int = 6              # ĞœĞµÑÑÑ†ĞµĞ² Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ°
    
    # Model params
    random_state: int = 42
    n_estimators: int = 200
    
    # Output
    output_dir: str = "crash_analysis_results"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ Ğ”ĞĞĞĞ«Ğ¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_data(filepath: str) -> pd.DataFrame:
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° CSV Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°"""
    print(f"\nğŸ“‚ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· {filepath}...")
    
    df = pd.read_csv(filepath, parse_dates=['timestamp'])
    df = df.sort_values('timestamp').reset_index(drop=True)
    
    print(f"   Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(df):,} ÑÑ‚Ñ€Ğ¾Ğº")
    print(f"   ĞŸĞµÑ€Ğ¸Ğ¾Ğ´: {df['timestamp'].min()} â€” {df['timestamp'].max()}")
    print(f"   ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸: {list(df.columns)}")
    
    return df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FEATURE ENGINEERING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def calculate_zscore(series: pd.Series, window: int) -> pd.Series:
    """Ğ Ğ°ÑÑ‡Ñ‘Ñ‚ rolling Z-score"""
    rolling_mean = series.rolling(window=window, min_periods=window//2).mean()
    rolling_std = series.rolling(window=window, min_periods=window//2).std()
    return (series - rolling_mean) / rolling_std.replace(0, np.nan)


def calculate_pct_change(series: pd.Series, periods: int) -> pd.Series:
    """Ğ Ğ°ÑÑ‡Ñ‘Ñ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ"""
    return series.pct_change(periods=periods) * 100


def calculate_delta(series: pd.Series, periods: int) -> pd.Series:
    """Ğ Ğ°ÑÑ‡Ñ‘Ñ‚ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ"""
    return series.diff(periods=periods)


def engineer_features(df: pd.DataFrame, config: PipelineConfig) -> pd.DataFrame:
    """
    Feature Engineering
    
    Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ ML Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.
    """
    print("\nğŸ”§ Feature Engineering...")
    
    df = df.copy()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ‘ĞĞ—ĞĞ’Ğ«Ğ• ĞŸĞ Ğ˜Ğ—ĞĞĞšĞ˜ (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # Taker Delta Ratio (Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ sell/buy Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ²)
    if 'takerSellVolume' in df.columns and 'takerBuyVolume' in df.columns:
        df['taker_delta_ratio'] = df['takerSellVolume'] / df['takerBuyVolume'].replace(0, np.nan)
        df['taker_imbalance'] = (df['takerSellVolume'] - df['takerBuyVolume']) / (
            df['takerSellVolume'] + df['takerBuyVolume']
        ).replace(0, np.nan)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Z-SCORES (Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # OI Z-score 24h
    if 'openInterest' in df.columns:
        df['oi_zscore_24h'] = calculate_zscore(df['openInterest'], config.zscore_24h_bars)
        df['oi_change_pct_4h'] = calculate_pct_change(df['openInterest'], 16)
        df['oi_change_pct_24h'] = calculate_pct_change(df['openInterest'], config.zscore_24h_bars)
    
    # Funding Z-score 24h
    if 'fundingRate' in df.columns:
        df['funding_zscore_24h'] = calculate_zscore(df['fundingRate'], config.zscore_24h_bars)
        # Ğ­ĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ funding (> 0.1% Ğ·Ğ° 8h = Ğ¿ĞµÑ€ĞµĞ³Ñ€ĞµÑ‚Ñ‹Ğ¹ Ğ»Ğ¾Ğ½Ğ³)
        df['funding_extreme'] = (df['fundingRate'].abs() > 0.001).astype(int)
    
    # ATR Z-score 72h (Ğ²Ğ¾Ğ»Ğ°Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ¾Ñ€Ğ¼Ñ‹)
    if 'atr14' in df.columns:
        df['atr_zscore_72h'] = calculate_zscore(df['atr14'], config.zscore_72h_bars)
        df['atr_expanding'] = (df['atr14'] > df['atr14'].rolling(24).mean()).astype(int)
    
    # Volume Z-score 24h
    if 'priceVolume' in df.columns:
        df['volume_zscore_24h'] = calculate_zscore(df['priceVolume'], config.zscore_24h_bars)
        df['volume_spike'] = (df['volume_zscore_24h'] > 2).astype(int)
    
    # Fear & Greed Z-score 7d
    if 'fearGreedIndex' in df.columns:
        df['feargreed_zscore_7d'] = calculate_zscore(df['fearGreedIndex'], config.zscore_7d_bars)
        df['feargreed_extreme_fear'] = (df['fearGreedIndex'] <= 20).astype(int)
        df['feargreed_extreme_greed'] = (df['fearGreedIndex'] >= 80).astype(int)
    
    # Basis Z-score 24h
    if 'spotFuturesBasis' in df.columns:
        df['basis_zscore_24h'] = calculate_zscore(df['spotFuturesBasis'], config.zscore_24h_bars)
        df['basis_negative'] = (df['spotFuturesBasis'] < 0).astype(int)
        df['basis_extreme'] = (df['spotFuturesBasis'].abs() > df['spotFuturesBasis'].abs().quantile(0.9)).astype(int)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ¦Ğ•ĞĞĞ’Ğ«Ğ• ĞŸĞ Ğ˜Ğ—ĞĞĞšĞ˜
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    if 'priceClose' in df.columns:
        # Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ½Ñ‹ Ğ·Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ñ‹
        df['price_change_pct_1h'] = calculate_pct_change(df['priceClose'], 4)
        df['price_change_pct_4h'] = calculate_pct_change(df['priceClose'], 16)
        df['price_change_pct_24h'] = calculate_pct_change(df['priceClose'], 96)
        
        # Ğ Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼Ğ°
        df['price_from_high_24h'] = (
            df['priceClose'] / df['priceClose'].rolling(96).max() - 1
        ) * 100
        
        # Ğ Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ°
        df['price_from_low_24h'] = (
            df['priceClose'] / df['priceClose'].rolling(96).min() - 1
        ) * 100
        
        # RSI-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€
        delta = df['priceClose'].diff()
        gain = delta.where(delta > 0, 0).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss.replace(0, np.nan)
        df['rsi_14'] = 100 - (100 / (1 + rs))
        
        # ĞŸĞµÑ€ĞµĞºÑƒĞ¿Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ/Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ğ¾Ğ´Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ
        df['rsi_overbought'] = (df['rsi_14'] > 70).astype(int)
        df['rsi_oversold'] = (df['rsi_14'] < 30).astype(int)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ĞšĞĞœĞ‘Ğ˜ĞĞ˜Ğ ĞĞ’ĞĞĞĞ«Ğ• ĞŸĞ Ğ˜Ğ—ĞĞĞšĞ˜
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # ĞŸÑ€Ğ¸Ğ·Ğ½Ğ°Ğº "Ğ¿ĞµÑ€ĞµĞ³Ñ€ĞµÑ‚Ñ‹Ğ¹ Ñ€Ñ‹Ğ½Ğ¾Ğº"
    overheated_conditions = []
    if 'funding_extreme' in df.columns:
        overheated_conditions.append(df['funding_extreme'])
    if 'feargreed_extreme_greed' in df.columns:
        overheated_conditions.append(df['feargreed_extreme_greed'])
    if 'rsi_overbought' in df.columns:
        overheated_conditions.append(df['rsi_overbought'])
    
    if overheated_conditions:
        df['market_overheated'] = sum(overheated_conditions)
    
    # ĞŸÑ€Ğ¸Ğ·Ğ½Ğ°Ğº "Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ"
    pressure_conditions = []
    if 'oi_zscore_24h' in df.columns:
        pressure_conditions.append((df['oi_zscore_24h'] > 1.5).astype(int))
    if 'volume_spike' in df.columns:
        pressure_conditions.append(df['volume_spike'])
    if 'atr_expanding' in df.columns:
        pressure_conditions.append(df['atr_expanding'])
    
    if pressure_conditions:
        df['pressure_building'] = sum(pressure_conditions)
    
    # ĞŸÑ€Ğ¸Ğ·Ğ½Ğ°Ğº "Ğ¼ĞµĞ´Ğ²ĞµĞ¶ÑŒÑ Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ñ" (Ñ†ĞµĞ½Ğ° Ñ€Ğ°ÑÑ‚Ñ‘Ñ‚, Ğ½Ğ¾ OI Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚)
    if 'price_change_pct_4h' in df.columns and 'oi_change_pct_4h' in df.columns:
        df['bearish_divergence'] = (
            (df['price_change_pct_4h'] > 1) & (df['oi_change_pct_4h'] < -2)
        ).astype(int)
    
    print(f"   Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¾ {len([c for c in df.columns if c not in ['timestamp']])} Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²")
    
    return df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TARGET VARIABLE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def calculate_target(df: pd.DataFrame, config: PipelineConfig) -> pd.DataFrame:
    """
    Ğ Ğ°ÑÑ‡Ñ‘Ñ‚ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ crash_next_Xh
    
    crash = 1, ĞµÑĞ»Ğ¸ Ğ² ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… N Ğ±Ğ°Ñ€Ğ°Ñ… Ñ†ĞµĞ½Ğ° Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ½Ğ° X% Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ
    """
    print(f"\nğŸ¯ Ğ Ğ°ÑÑ‡Ñ‘Ñ‚ target: crash â‰¥{config.crash_threshold_pct}% Ğ² Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ {config.crash_window_bars} Ğ±Ğ°Ñ€Ğ¾Ğ²...")
    
    df = df.copy()
    
    # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ†ĞµĞ½Ñƒ Ğ² ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… N Ğ±Ğ°Ñ€Ğ°Ñ…
    df['future_min_price'] = df['priceClose'].shift(-1).rolling(
        window=config.crash_window_bars,
        min_periods=1
    ).min().shift(-config.crash_window_bars + 1)
    
    # Ğ Ğ°ÑÑ‡Ñ‘Ñ‚ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ
    df['future_drawdown_pct'] = (
        (df['future_min_price'] - df['priceClose']) / df['priceClose'] * 100
    )
    
    # Target: 1 ĞµÑĞ»Ğ¸ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ >= threshold
    df['crash_target'] = (df['future_drawdown_pct'] <= -config.crash_threshold_pct).astype(int)
    
    # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ N Ğ±Ğ°Ñ€Ğ¾Ğ² (Ğ½ĞµÑ‚ future data)
    df.loc[df.index[-config.crash_window_bars:], 'crash_target'] = np.nan
    
    # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    valid_rows = df['crash_target'].notna()
    crash_rate = df.loc[valid_rows, 'crash_target'].mean()
    crash_count = df.loc[valid_rows, 'crash_target'].sum()
    
    print(f"   Ğ’ÑĞµĞ³Ğ¾ crash ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹: {int(crash_count):,} Ğ¸Ğ· {valid_rows.sum():,} ({crash_rate*100:.2f}%)")
    
    return df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA PREPARATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def prepare_data(
    df: pd.DataFrame,
    config: PipelineConfig
) -> Tuple[pd.DataFrame, List[str]]:
    """
    ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
    """
    print("\nğŸ“Š ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...")
    
    # Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    feature_cols = [
        col for col in df.columns 
        if col not in [
            'timestamp', 'priceClose', 'priceVolume', 'openInterest', 
            'fundingRate', 'takerBuyVolume', 'takerSellVolume', 'atr14',
            'fearGreedIndex', 'spotFuturesBasis',
            'future_min_price', 'future_drawdown_pct', 'crash_target',
            # Ğ˜ÑĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ raw ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸
            'bidDepthDeltaPct1h', 'exchangeInflowZscore24h', 'reserveDelta24h',
            'liquidationDensityRatio', 'crashNext6h'
        ]
    ]
    
    # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ NaN
    for col in feature_cols.copy():
        if df[col].isna().mean() > 0.5:
            feature_cols.remove(col)
            print(f"   âš ï¸  Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ° {col} (>{50}% NaN)")
    
    print(f"   Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ {len(feature_cols)} Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²")
    
    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ñ NaN Ğ² target
    df = df.dropna(subset=['crash_target'])
    
    # Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ NaN Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ñ…
    for col in feature_cols:
        if df[col].isna().any():
            df[col] = df[col].fillna(df[col].median())
    
    print(f"   Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚: {len(df):,} ÑÑ‚Ñ€Ğ¾Ğº")
    
    return df, feature_cols


def walk_forward_split(
    df: pd.DataFrame,
    config: PipelineConfig
) -> List[Tuple[pd.DataFrame, pd.DataFrame]]:
    """
    Walk-forward split Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²
    
    ĞĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ random split â€” ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ´Ñ‘Ñ‚ Ğº look-ahead bias!
    """
    print("\nğŸ“… Walk-forward split...")
    
    splits = []
    
    df = df.sort_values('timestamp').reset_index(drop=True)
    
    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ñ‹
    min_date = df['timestamp'].min()
    max_date = df['timestamp'].max()
    
    # ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ñ train_years Ğ»ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    train_end = min_date + pd.DateOffset(years=config.train_years)
    
    while train_end + pd.DateOffset(months=config.test_months) <= max_date:
        test_end = train_end + pd.DateOffset(months=config.test_months)
        
        train_mask = df['timestamp'] < train_end
        test_mask = (df['timestamp'] >= train_end) & (df['timestamp'] < test_end)
        
        train_df = df[train_mask].copy()
        test_df = df[test_mask].copy()
        
        if len(train_df) > 1000 and len(test_df) > 100:
            splits.append((train_df, test_df))
            print(f"   Split {len(splits)}: Train {train_df['timestamp'].min().date()} â€” {train_df['timestamp'].max().date()} ({len(train_df):,} rows)")
            print(f"            Test  {test_df['timestamp'].min().date()} â€” {test_df['timestamp'].max().date()} ({len(test_df):,} rows)")
        
        # Ğ¡Ğ´Ğ²Ğ¸Ğ³Ğ°ĞµĞ¼ Ğ¾ĞºĞ½Ğ¾
        train_end += pd.DateOffset(months=config.test_months)
    
    print(f"\n   Ğ’ÑĞµĞ³Ğ¾ {len(splits)} fold(s)")
    
    return splits


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞœĞĞ”Ğ•Ğ›Ğ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class ModelResult:
    """Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    name: str
    model: object
    roc_auc: float
    precision: float
    recall: float
    f1: float
    feature_importance: Optional[pd.DataFrame] = None
    y_pred_proba: Optional[np.ndarray] = None


def train_logistic_regression(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    feature_names: List[str],
    config: PipelineConfig
) -> ModelResult:
    """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Logistic Regression (baseline)"""
    
    model = LogisticRegression(
        class_weight='balanced',
        max_iter=1000,
        random_state=config.random_state
    )
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
    
    # Feature importance (ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ñ‹)
    importance = pd.DataFrame({
        'feature': feature_names,
        'importance': np.abs(model.coef_[0])
    }).sort_values('importance', ascending=False)
    
    return ModelResult(
        name="Logistic Regression",
        model=model,
        roc_auc=roc_auc,
        precision=report.get('1', {}).get('precision', 0),
        recall=report.get('1', {}).get('recall', 0),
        f1=report.get('1', {}).get('f1-score', 0),
        feature_importance=importance,
        y_pred_proba=y_pred_proba
    )


def train_random_forest(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    feature_names: List[str],
    config: PipelineConfig
) -> ModelResult:
    """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Random Forest"""
    
    model = RandomForestClassifier(
        n_estimators=config.n_estimators,
        class_weight='balanced',
        max_depth=10,
        min_samples_leaf=20,
        random_state=config.random_state,
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
    
    importance = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    return ModelResult(
        name="Random Forest",
        model=model,
        roc_auc=roc_auc,
        precision=report.get('1', {}).get('precision', 0),
        recall=report.get('1', {}).get('recall', 0),
        f1=report.get('1', {}).get('f1-score', 0),
        feature_importance=importance,
        y_pred_proba=y_pred_proba
    )


def train_xgboost(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    feature_names: List[str],
    config: PipelineConfig
) -> Optional[ModelResult]:
    """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ XGBoost"""
    
    if not HAS_XGBOOST:
        return None
    
    # Ğ Ğ°ÑÑ‡Ñ‘Ñ‚ scale_pos_weight Ğ´Ğ»Ñ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° ĞºĞ»Ğ°ÑÑĞ¾Ğ²
    scale_pos_weight = (y_train == 0).sum() / max((y_train == 1).sum(), 1)
    
    model = xgb.XGBClassifier(
        n_estimators=config.n_estimators,
        max_depth=6,
        learning_rate=0.05,
        scale_pos_weight=scale_pos_weight,
        random_state=config.random_state,
        use_label_encoder=False,
        eval_metric='logloss',
        n_jobs=-1
    )
    model.fit(X_train, y_train, verbose=False)
    
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
    
    importance = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    return ModelResult(
        name="XGBoost",
        model=model,
        roc_auc=roc_auc,
        precision=report.get('1', {}).get('precision', 0),
        recall=report.get('1', {}).get('recall', 0),
        f1=report.get('1', {}).get('f1-score', 0),
        feature_importance=importance,
        y_pred_proba=y_pred_proba
    )


def train_lightgbm(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    feature_names: List[str],
    config: PipelineConfig
) -> Optional[ModelResult]:
    """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LightGBM"""
    
    if not HAS_LIGHTGBM:
        return None
    
    model = lgb.LGBMClassifier(
        n_estimators=config.n_estimators,
        max_depth=6,
        learning_rate=0.05,
        class_weight='balanced',
        random_state=config.random_state,
        verbose=-1,
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
    
    importance = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    return ModelResult(
        name="LightGBM",
        model=model,
        roc_auc=roc_auc,
        precision=report.get('1', {}).get('precision', 0),
        recall=report.get('1', {}).get('recall', 0),
        f1=report.get('1', {}).get('f1-score', 0),
        feature_importance=importance,
        y_pred_proba=y_pred_proba
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SHAP ANALYSIS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def analyze_shap(
    model: object,
    X_test: np.ndarray,
    feature_names: List[str],
    output_dir: Path,
    model_name: str
) -> Optional[pd.DataFrame]:
    """SHAP Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    
    if not HAS_SHAP:
        print("   âš ï¸  SHAP Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·")
        return None
    
    print(f"\nğŸ” SHAP Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ»Ñ {model_name}...")
    
    try:
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ explainer
        if hasattr(model, 'feature_importances_'):
            explainer = shap.TreeExplainer(model)
        else:
            explainer = shap.LinearExplainer(model, X_test[:1000])
        
        # Ğ Ğ°ÑÑ‡Ñ‘Ñ‚ SHAP values (Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸)
        sample_size = min(1000, len(X_test))
        X_sample = X_test[:sample_size]
        shap_values = explainer.shap_values(X_sample)
        
        # Ğ”Ğ»Ñ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ±ĞµÑ€Ñ‘Ğ¼ ĞºĞ»Ğ°ÑÑ 1
        if isinstance(shap_values, list):
            shap_values = shap_values[1]
        
        # Summary plot
        plt.figure(figsize=(12, 8))
        shap.summary_plot(
            shap_values, 
            X_sample, 
            feature_names=feature_names,
            show=False
        )
        plt.tight_layout()
        plt.savefig(output_dir / f'shap_summary_{model_name.lower().replace(" ", "_")}.png', dpi=150)
        plt.close()
        
        # Feature importance from SHAP
        shap_importance = pd.DataFrame({
            'feature': feature_names,
            'shap_importance': np.abs(shap_values).mean(axis=0)
        }).sort_values('shap_importance', ascending=False)
        
        print(f"   Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½ SHAP summary plot")
        
        return shap_importance
        
    except Exception as e:
        print(f"   âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° SHAP: {e}")
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RULE EXTRACTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_high_risk_rules(
    df: pd.DataFrame,
    feature_cols: List[str],
    target_col: str = 'crash_target',
    min_probability: float = 0.15,  # ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°
    min_support: int = 50           # ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²
) -> List[Dict]:
    """
    Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ¸ÑĞºĞ° crash
    
    ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ crash Ğ²Ñ‹ÑˆĞµ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ°.
    """
    print(f"\nğŸ“‹ Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» (P(crash) > {min_probability*100:.0f}%, support â‰¥ {min_support})...")
    
    rules = []
    base_crash_rate = df[target_col].mean()
    
    # ĞĞ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ
    for col in feature_cols:
        if df[col].dtype in ['float64', 'float32', 'int64', 'int32']:
            # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»Ğ¸
            for q in [0.1, 0.2, 0.3, 0.7, 0.8, 0.9]:
                threshold = df[col].quantile(q)
                
                # Ğ£ÑĞ»Ğ¾Ğ²Ğ¸Ğµ >
                mask = df[col] > threshold
                if mask.sum() >= min_support:
                    prob = df.loc[mask, target_col].mean()
                    lift = prob / base_crash_rate if base_crash_rate > 0 else 0
                    
                    if prob >= min_probability and lift > 1.5:
                        rules.append({
                            'conditions': [(col, '>', threshold)],
                            'probability': prob,
                            'support': mask.sum(),
                            'lift': lift
                        })
                
                # Ğ£ÑĞ»Ğ¾Ğ²Ğ¸Ğµ <
                mask = df[col] < threshold
                if mask.sum() >= min_support:
                    prob = df.loc[mask, target_col].mean()
                    lift = prob / base_crash_rate if base_crash_rate > 0 else 0
                    
                    if prob >= min_probability and lift > 1.5:
                        rules.append({
                            'conditions': [(col, '<', threshold)],
                            'probability': prob,
                            'support': mask.sum(),
                            'lift': lift
                        })
    
    # ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· 2 ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ (Ñ‚Ğ¾Ğ¿ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»)
    rules.sort(key=lambda x: -x['lift'])
    top_rules = rules[:20]
    
    combined_rules = []
    for i, rule1 in enumerate(top_rules):
        for rule2 in top_rules[i+1:]:
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ
            if rule1['conditions'][0][0] != rule2['conditions'][0][0]:
                # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ¾Ğ±Ğ° ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ
                cond1 = rule1['conditions'][0]
                cond2 = rule2['conditions'][0]
                
                if cond1[1] == '>':
                    mask1 = df[cond1[0]] > cond1[2]
                else:
                    mask1 = df[cond1[0]] < cond1[2]
                
                if cond2[1] == '>':
                    mask2 = df[cond2[0]] > cond2[2]
                else:
                    mask2 = df[cond2[0]] < cond2[2]
                
                combined_mask = mask1 & mask2
                
                if combined_mask.sum() >= min_support:
                    prob = df.loc[combined_mask, target_col].mean()
                    lift = prob / base_crash_rate if base_crash_rate > 0 else 0
                    
                    if prob >= min_probability and lift > 2.0:
                        combined_rules.append({
                            'conditions': [cond1, cond2],
                            'probability': prob,
                            'support': combined_mask.sum(),
                            'lift': lift
                        })
    
    # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ¸ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼
    all_rules = rules + combined_rules
    all_rules.sort(key=lambda x: (-x['lift'], -x['probability']))
    
    print(f"   ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(all_rules)} Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»")
    
    return all_rules[:30]  # Ğ¢Ğ¾Ğ¿-30 Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»


def format_rules(rules: List[Dict], base_crash_rate: float) -> str:
    """Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ² Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ²Ğ¸Ğ´"""
    
    output = []
    output.append("\n" + "="*80)
    output.append("ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ Ğ’Ğ«Ğ¡ĞĞšĞĞ“Ğ Ğ Ğ˜Ğ¡ĞšĞ CRASH")
    output.append(f"Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ crash: {base_crash_rate*100:.2f}%")
    output.append("="*80)
    
    for i, rule in enumerate(rules, 1):
        conditions_str = " Ğ˜ ".join([
            f"{c[0]} {c[1]} {c[2]:.4f}" for c in rule['conditions']
        ])
        
        output.append(f"\n#{i}. {conditions_str}")
        output.append(f"    â†’ P(crash) = {rule['probability']*100:.1f}%")
        output.append(f"    â†’ Lift = {rule['lift']:.2f}x")
        output.append(f"    â†’ Support = {rule['support']} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²")
    
    return "\n".join(output)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ’Ğ˜Ğ—Ğ£ĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def plot_results(
    results: List[ModelResult],
    y_test: np.ndarray,
    output_dir: Path
):
    """Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²"""
    
    print("\nğŸ“Š Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹...")
    
    # ROC Curves
    plt.figure(figsize=(10, 8))
    for result in results:
        if result.y_pred_proba is not None:
            fpr, tpr, _ = roc_curve(y_test, result.y_pred_proba)
            plt.plot(fpr, tpr, label=f'{result.name} (AUC={result.roc_auc:.3f})')
    
    plt.plot([0, 1], [0, 1], 'k--', label='Random')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'roc_curves.png', dpi=150)
    plt.close()
    
    # Precision-Recall Curves
    plt.figure(figsize=(10, 8))
    for result in results:
        if result.y_pred_proba is not None:
            precision, recall, _ = precision_recall_curve(y_test, result.y_pred_proba)
            plt.plot(recall, precision, label=f'{result.name}')
    
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curves')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'precision_recall_curves.png', dpi=150)
    plt.close()
    
    # Feature Importance (Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ)
    best_result = max(results, key=lambda x: x.roc_auc)
    if best_result.feature_importance is not None:
        plt.figure(figsize=(12, 10))
        top_features = best_result.feature_importance.head(20)
        plt.barh(range(len(top_features)), top_features['importance'].values)
        plt.yticks(range(len(top_features)), top_features['feature'].values)
        plt.xlabel('Importance')
        plt.title(f'Top 20 Feature Importance ({best_result.name})')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.savefig(output_dir / 'feature_importance.png', dpi=150)
        plt.close()
    
    print(f"   Ğ“Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ² {output_dir}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN PIPELINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_pipeline(input_file: str, config: PipelineConfig):
    """Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ ML Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°"""
    
    print("\n" + "="*80)
    print("ğŸš€ BTC CRASH PREDICTION ML PIPELINE")
    print("="*80)
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ output Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ
    output_dir = Path(config.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # 1. Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    df = load_data(input_file)
    
    # 2. Feature Engineering
    df = engineer_features(df, config)
    
    # 3. Target Variable
    df = calculate_target(df, config)
    
    # 4. ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    df, feature_cols = prepare_data(df, config)
    
    # 5. Walk-forward splits
    splits = walk_forward_split(df, config)
    
    if not splits:
        print("âŒ ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ walk-forward validation")
        return
    
    # 6. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ fold
    all_results = []
    
    for fold_idx, (train_df, test_df) in enumerate(splits):
        print(f"\n{'='*40}")
        print(f"FOLD {fold_idx + 1}/{len(splits)}")
        print(f"{'='*40}")
        
        # Prepare features
        X_train = train_df[feature_cols].values
        y_train = train_df['crash_target'].values
        X_test = test_df[feature_cols].values
        y_test = test_df['crash_target'].values
        
        # Normalize
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Train models
        results = []
        
        print("\nğŸ¤– ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹...")
        
        # Logistic Regression
        lr_result = train_logistic_regression(
            X_train_scaled, y_train, X_test_scaled, y_test, feature_cols, config
        )
        results.append(lr_result)
        print(f"   {lr_result.name}: AUC={lr_result.roc_auc:.3f}, Precision={lr_result.precision:.3f}, Recall={lr_result.recall:.3f}")
        
        # Random Forest
        rf_result = train_random_forest(
            X_train_scaled, y_train, X_test_scaled, y_test, feature_cols, config
        )
        results.append(rf_result)
        print(f"   {rf_result.name}: AUC={rf_result.roc_auc:.3f}, Precision={rf_result.precision:.3f}, Recall={rf_result.recall:.3f}")
        
        # XGBoost
        xgb_result = train_xgboost(
            X_train_scaled, y_train, X_test_scaled, y_test, feature_cols, config
        )
        if xgb_result:
            results.append(xgb_result)
            print(f"   {xgb_result.name}: AUC={xgb_result.roc_auc:.3f}, Precision={xgb_result.precision:.3f}, Recall={xgb_result.recall:.3f}")
        
        # LightGBM
        lgb_result = train_lightgbm(
            X_train_scaled, y_train, X_test_scaled, y_test, feature_cols, config
        )
        if lgb_result:
            results.append(lgb_result)
            print(f"   {lgb_result.name}: AUC={lgb_result.roc_auc:.3f}, Precision={lgb_result.precision:.3f}, Recall={lgb_result.recall:.3f}")
        
        all_results.append((fold_idx, results, y_test))
    
    # 7. Aggregate results
    print("\n" + "="*80)
    print("ğŸ“ˆ Ğ˜Ğ¢ĞĞ“ĞĞ’Ğ«Ğ• Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ« (ÑƒÑÑ€ĞµĞ´Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ folds)")
    print("="*80)
    
    model_names = set()
    for _, results, _ in all_results:
        for r in results:
            model_names.add(r.name)
    
    for model_name in sorted(model_names):
        aucs = []
        precisions = []
        recalls = []
        
        for _, results, _ in all_results:
            for r in results:
                if r.name == model_name:
                    aucs.append(r.roc_auc)
                    precisions.append(r.precision)
                    recalls.append(r.recall)
        
        if aucs:
            print(f"\n{model_name}:")
            print(f"   ROC AUC:   {np.mean(aucs):.3f} Â± {np.std(aucs):.3f}")
            print(f"   Precision: {np.mean(precisions):.3f} Â± {np.std(precisions):.3f}")
            print(f"   Recall:    {np.mean(recalls):.3f} Â± {np.std(recalls):.3f}")
    
    # 8. SHAP analysis Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¼ fold
    last_fold_idx, last_results, last_y_test = all_results[-1]
    best_result = max(last_results, key=lambda x: x.roc_auc)
    
    # Prepare test data for SHAP
    _, test_df = splits[-1]
    X_test = test_df[feature_cols].values
    scaler = StandardScaler()
    X_train_last = splits[-1][0][feature_cols].values
    scaler.fit(X_train_last)
    X_test_scaled = scaler.transform(X_test)
    
    shap_importance = analyze_shap(
        best_result.model,
        X_test_scaled,
        feature_cols,
        output_dir,
        best_result.name
    )
    
    # 9. Extract rules
    rules = extract_high_risk_rules(df, feature_cols)
    base_crash_rate = df['crash_target'].mean()
    rules_text = format_rules(rules, base_crash_rate)
    print(rules_text)
    
    # Save rules to file
    with open(output_dir / 'crash_rules.txt', 'w', encoding='utf-8') as f:
        f.write(rules_text)
    
    # 10. Visualizations
    plot_results(last_results, last_y_test, output_dir)
    
    # 11. Summary
    print("\n" + "="*80)
    print("âœ… ĞĞĞĞ›Ğ˜Ğ— Ğ—ĞĞ’Ğ•Ğ Ğ¨ĞĞ")
    print("="*80)
    print(f"\nĞ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ²: {output_dir.absolute()}")
    print(f"  - roc_curves.png")
    print(f"  - precision_recall_curves.png")
    print(f"  - feature_importance.png")
    print(f"  - crash_rules.txt")
    if HAS_SHAP:
        print(f"  - shap_summary_*.png")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(
        description='BTC Crash Prediction ML Pipeline',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ:

  # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº
  python crash_ml_pipeline.py --input crash_data.csv

  # Ğ¡ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸
  python crash_ml_pipeline.py --input crash_data.csv --crash-threshold 7 --crash-window 24

  # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
  python crash_ml_pipeline.py --input crash_data.csv --output-dir my_results
        """
    )
    
    parser.add_argument(
        '--input', '-i',
        type=str,
        required=True,
        help='ĞŸÑƒÑ‚ÑŒ Ğº CSV Ñ„Ğ°Ğ¹Ğ»Ñƒ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸'
    )
    
    parser.add_argument(
        '--crash-threshold',
        type=float,
        default=5.0,
        help='ĞŸĞ¾Ñ€Ğ¾Ğ³ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ½Ñ‹ Ğ´Ğ»Ñ crash (%%). Default: 5.0'
    )
    
    parser.add_argument(
        '--crash-window',
        type=int,
        default=48,
        help='ĞĞºĞ½Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° crash (Ğ² Ğ±Ğ°Ñ€Ğ°Ñ… Ğ¿Ğ¾ 15m). Default: 48 (12 Ñ‡Ğ°ÑĞ¾Ğ²)'
    )
    
    parser.add_argument(
        '--train-years',
        type=int,
        default=2,
        help='ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ»ĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Default: 2'
    )
    
    parser.add_argument(
        '--output-dir', '-o',
        type=str,
        default='crash_analysis_results',
        help='Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Default: crash_analysis_results'
    )
    
    args = parser.parse_args()
    
    config = PipelineConfig(
        crash_threshold_pct=args.crash_threshold,
        crash_window_bars=args.crash_window,
        train_years=args.train_years,
        output_dir=args.output_dir
    )
    
    run_pipeline(args.input, config)


if __name__ == '__main__':
    main()
